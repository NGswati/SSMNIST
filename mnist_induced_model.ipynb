{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"MNIST Parameters\")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\",\n",
    "    type=int,\n",
    "    default=50,\n",
    "    metavar=\"N\",\n",
    "    help=\"input batch size for training (default: 50)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--epochs\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    metavar=\"N\",\n",
    "    help=\"number of epochs to train (default: 50)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr\", type=float, default=0.01, metavar=\"LR\", help=\"learning rate (default: 0.01)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--momentum\",\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    metavar=\"M\",\n",
    "    help=\"SGD momentum (default:0.5)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default:1)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dropout_rate\",\n",
    "    type=float,\n",
    "    default=0.5,\n",
    "    metavar=\"DO\",\n",
    "    help=\"dropout rate (default: 0.5)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_file\",\n",
    "    type=str,\n",
    "    default=\"output\",\n",
    "    metavar=\"OF\",\n",
    "    help=\"output file (default: output)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_norm\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    metavar=\"BN\",\n",
    "    help=\"batch nomralization (default: 1)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ada_delta\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    metavar=\"AD\",\n",
    "    help=\"adaptive learning rate (default: 1)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_aug\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    metavar=\"DA\",\n",
    "    help=\"data augmentation (default: 1)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--init_round\",\n",
    "    type=int,\n",
    "    default=15,\n",
    "    metavar=\"IR\",\n",
    "    help=\"initialization round (default: 15)\",\n",
    ")\n",
    "args = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 50, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(50, 50, kernel_size=5, padding=2)\n",
    "        self.conv2_drop = nn.Dropout2d(p=args.dropout_rate)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(50)\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(50)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 50, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if args.batch_norm:\n",
    "            x = self.batch_norm_1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv1(x)), 2))\n",
    "        if args.batch_norm:\n",
    "            x = self.batch_norm_2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        if args.batch_norm:\n",
    "            x = self.batch_norm_3(x)\n",
    "        x = x.view(args.batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/subMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/subMNIST/raw/train-images-idx3-ubyte.gz to ./data/subMNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/subMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/subMNIST/raw/train-labels-idx1-ubyte.gz to ./data/subMNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/subMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/subMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/subMNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/subMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/subMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/subMNIST/raw\n",
      "\n",
      "3000\n",
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "class subMNIST(MNIST):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        train=True,\n",
    "        transform=None,\n",
    "        target_transform=None,\n",
    "        download=False,\n",
    "        k=3000,\n",
    "    ):\n",
    "        super(subMNIST, self).__init__(\n",
    "            root, train, transform, target_transform, download\n",
    "        )\n",
    "        self.k = k\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return self.k\n",
    "        else:\n",
    "            return 10000\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "trainset = subMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=4, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loading data!\")\n",
    "trainset_labeled = pickle.load(open(\"train_labeled.p\", \"rb\"))\n",
    "validset = pickle.load(open(\"validation.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(s):\n",
    "    data = s.train_data\n",
    "    label = s.train_labels\n",
    "    new_data = np.ndarray(shape=(0, 28, 28), dtype=\"uint8\")\n",
    "    new_label = torch.LongTensor(0, 1)\n",
    "    n = 6\n",
    "    for i in range(len(s)):\n",
    "        for k in range(1, n + 1):\n",
    "            tmp = data[i].numpy()\n",
    "            new1 = misc.imrotate(tmp, k * 5)\n",
    "            new2 = misc.imrotate(tmp, 360 - (k * 5))\n",
    "            new_data = np.append(new_data, [new1, new2], axis=0)\n",
    "        new_label = torch.cat(\n",
    "            (new_label, torch.LongTensor(2 * n, 1).fill_(label[i])), 0\n",
    "        )\n",
    "    s.train_data = torch.cat((s.train_data, torch.from_numpy(new_data)), 0)\n",
    "    s.train_labels = torch.cat((s.train_labels, new_label), 0).view(-1)\n",
    "    s.k = len(s.train_labels)\n",
    "    return s\n",
    "\n",
    "\n",
    "if args.data_aug:\n",
    "    trainset_labeled = data_augment(trainset_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset_labeled, batch_size=args.batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    validset, batch_size=args.batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "\n",
    "trainset_unlabeled = pickle.load(open(\"train_unlabeled.p\", \"rb\"))\n",
    "trainset_unlabeled.train_labels = torch.Tensor(len(trainset_unlabeled)).fill_(-1)\n",
    "train_unlabeled_loader = torch.utils.data.DataLoader(\n",
    "    trainset_unlabeled, batch_size=50, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 50, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(50, 50, kernel_size=5, padding=2)\n",
    "        self.conv2_drop = nn.Dropout2d(p=args.dropout_rate)\n",
    "        self.batch_norm_1 = nn.BatchNorm2d(1)\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(50)\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(50)\n",
    "        self.fc1 = nn.Linear(7 * 7 * 50, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if args.batch_norm:\n",
    "            x = self.batch_norm_1(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv1(x)), 2))\n",
    "        if args.batch_norm:\n",
    "            x = self.batch_norm_2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        if args.batch_norm:\n",
    "            x = self.batch_norm_3(x)\n",
    "        x = x.view(args.batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "if args.ada_delta:\n",
    "    optimizer = optim.Adadelta(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    if epoch > args.init_round:\n",
    "        for batch_idx, (data, target) in enumerate(train_unlabeled_loader):\n",
    "            model.eval()\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            output = model(data)\n",
    "            fake_target = Variable(\n",
    "                output.data.max(1)[1].view(-1)\n",
    "            )  \n",
    "            model.train()\n",
    "            data.volatile = False\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, fake_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_unlabeled_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_unlabeled_loader),\n",
    "                        loss.data[0],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    avg_train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.data[0],\n",
    "                )\n",
    "            )\n",
    "            avg_train_loss += loss.data[0]\n",
    "    avg_train_loss = avg_train_loss / (trainset_labeled.k / 500)\n",
    "    loss_compare.write(str(avg_train_loss) + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, valid_loader, best_rate, best_epoch, model_name):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1]  \n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader)  \n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(valid_loader.dataset),\n",
    "            100.0 * correct / len(valid_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    if best_rate < (100.0 * correct / len(valid_loader.dataset)):\n",
    "        best_rate = 100.0 * correct / len(valid_loader.dataset)\n",
    "        best_epoch = epoch\n",
    "        torch.save(model, model_name)\n",
    "    loss_compare.write(str(test_loss) + \"\\n\")\n",
    "    accuracy_compare.write(str(100.0 * correct / len(valid_loader.dataset)) + \"\\n\")\n",
    "    return best_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_compare = open(args.output_file + \"loss_comparasion.csv\", \"w\")\n",
    "loss_compare.write(\"Train_Loss,Validation_Loss\\n\")\n",
    "\n",
    "accuracy_compare = open(args.output_file + \"accuracy.csv\", \"w\")\n",
    "accuracy_compare.write(\"Validation_accuracy\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rate = 0\n",
    "best_epoch = 0\n",
    "model_name = args.output_file + \".cnn_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    best_rate = test(epoch, valid_loader, best_rate, best_epoch, model_name)\n",
    "    print(best_rate)\n",
    "loss_compare.close()\n",
    "accuracy_compare.close()\n",
    "print(\"best rate: \")\n",
    "print(str(bset_rate) + \"\\n\")\n",
    "print(\"best epoch: \")\n",
    "print(str(best_epoch) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
